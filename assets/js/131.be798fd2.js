(window.webpackJsonp=window.webpackJsonp||[]).push([[131],{452:function(s,t,n){"use strict";n.r(t);var a=n(3),e=Object(a.a)({},(function(){var s=this,t=s._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[t("h1",{attrs:{id:"gensim模块中lsi模型自然语言处理"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#gensim模块中lsi模型自然语言处理"}},[s._v("#")]),s._v(" gensim模块中Lsi模型自然语言处理")]),s._v(" "),t("ul",[t("li",[s._v("使用jieba分词处理词库，使用gensim中的Lsi模型训练语料库")])]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#!/usr/bin/env python")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# -*- coding: utf-8 -*-")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" jieba\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" gensim "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" corpora\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" gensim "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" models\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" gensim "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" similarities\n\nq_l "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"今天你冷吗"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"北京的雾霾去哪了"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"北京的天蓝吗"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"你叫什么名字"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"你今年几岁啦"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("  "),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 问题库")]),s._v("\nq_s "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"你今年多大了"')]),s._v("  "),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 问题样本")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 1.问题库分词处理")]),s._v("\nall_doc_list "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("for")]),s._v(" doc "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" q_l"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n    doc_list "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("word "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("for")]),s._v(" word "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" jieba"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("cut"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("doc"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n    all_doc_list"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("append"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("doc_list"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\ndoc_test_list "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("word "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("for")]),s._v(" word "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" jieba"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("cut"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("q_s"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# print(all_doc_list)")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# print(doc_test_list)")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[s._v("'''\n[['今天', '你', '冷', '吗'], ['北京', '的', '雾', '霾', '去', '哪', '了'], ['北京', '的', '天蓝', '吗'], ['你', '叫', '什么', '名字'], ['你', '今年', '几岁', '啦']]\n['你', '今年', '多大', '了']\n'''")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 2.制作语料库")]),s._v("\ndictionary "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" corpora"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Dictionary"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("all_doc_list"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("  "),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 制作词袋：就是将很多很多的词,进行排列形成一个 词(key) 与一个 标志位(value) 的字典")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v('# print("token2id", dictionary.token2id)')]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v('# print("dictionary", dictionary)')]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[s._v("'''\ntoken2id {'今天': 0, '你': 1, '冷': 2, '吗': 3, '了': 4, '北京': 5, '去': 6, '哪': 7, '的': 8, '雾': 9, '霾': 10, '天蓝': 11, '什么': 12, '叫': 13, '名字': 14, '今年': 15, '几岁': 16, '啦': 17}\ndictionary Dictionary(18 unique tokens: ['今天', '你', '冷', '吗', '了']...)\n'''")]),s._v("\n\n\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 2.1问题库的语料库")]),s._v("\ncorpus "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("dictionary"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("doc2bow"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("doc"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("for")]),s._v(" doc "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" all_doc_list"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 这里是将all_doc_list 中的每一个列表中的词语 与 dictionary 中的Key进行匹配得到一个匹配后的结果,例如['你', '今年', '几岁', '了']，然后根据上面得到的 token2id {'今天': 0, '你': 1, '冷': 2, '吗': 3, '了': 4, '北京': 5, '去': 6, '哪': 7, '的': 8, '雾': 9, '霾': 10, '天蓝': 11, '什么': 12, '叫': 13, '名字': 14, '今年': 15, '几岁': 16, '啦': 17}")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 就可以得到 [(1, 1), (4, 1), (15, 1), (16, 1)]")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 1代表的的是 你 1代表出现一次, 4代表的是 了  1代表出现了一次, 以此类推 15 = 今年 , 16 = 几岁")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v('# print("corpus", corpus, type(corpus))')]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[s._v("'''\ncorpus [[(0, 1), (1, 1), (2, 1), (3, 1)], [(4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1)], [(3, 1), (5, 1), (8, 1), (11, 1)], [(1, 1), (12, 1), (13, 1), (14, 1)], [(1, 1), (15, 1), (16, 1), (17, 1)]] <class 'list'>\n'''")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 2.2问题的语料库")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 将需要寻找相似度的分词列表 做成 语料库 doc_test_vec")]),s._v("\ndoc_test_vec "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" dictionary"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("doc2bow"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("doc_test_list"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v('# print("doc_test_vec", doc_test_vec, type(doc_test_vec))')]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[s._v("'''\ndoc_test_vec [(1, 1), (4, 1), (15, 1)] <class 'list'>\n'''")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 3. 将corpus语料库(初识语料库) 使用Lsi模型进行训练")]),s._v("\nlsi "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" models"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("LsiModel"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("corpus"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 4 文本相似度获取")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 4.1 利用稀疏矩阵相似度算法 将 问题库的 语料库corpus的训练结果作为初始值")]),s._v("\nindex "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" similarities"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("SparseMatrixSimilarity"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("lsi"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("corpus"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" num_features"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("len")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("dictionary"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keys"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[s._v("'''\n举个例子：\nlsi 就是很多月饼模子\nlsi[corpus] 把 问题库中的每一个问题都用模子压出一个月饼\nlsi[doc_test_vec] 把问题样本压出一个月饼，拿着这个月饼，通过一定的算法去与lsi[corpus]比较。\n'''")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 4.2 将 语料库doc_test_vec 在 语料库corpus的训练结果 中的 向量表示 与 语料库corpus的 向量表示 做矩阵相似度计算")]),s._v("\nsim "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" index"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("lsi"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("doc_test_vec"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v('# print("sim", sim, type(sim))')]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[s._v("'''\nsim [ 4.3786496e-01  3.3099478e-01 -1.4901161e-08  4.3786496e-01\n  8.7572986e-01] <class 'numpy.ndarray'>  \n计算结果是：问题样本与问题库中的每一个问题匹配度的评分，根据评分就可以取到相似度最高问题\n'''")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 5.排序，获取结果")]),s._v("\ncc "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("sorted")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("enumerate")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("sim"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" key"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("lambda")]),s._v(" item"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("-")]),s._v("item"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# print(cc)")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[s._v("'''[(4, 0.87572986), (0, 0.43786496), (3, 0.43786496), (1, 0.33099478), (2, -1.4901161e-08)]'''")]),s._v("\n\nret "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" q_l"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("cc"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("' 问题样本:'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" q_s"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'\\n'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'匹配结果:'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" ret"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[s._v("'''你今年多大了 你今年几岁啦'''")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br"),t("span",{staticClass:"line-number"},[s._v("10")]),t("br"),t("span",{staticClass:"line-number"},[s._v("11")]),t("br"),t("span",{staticClass:"line-number"},[s._v("12")]),t("br"),t("span",{staticClass:"line-number"},[s._v("13")]),t("br"),t("span",{staticClass:"line-number"},[s._v("14")]),t("br"),t("span",{staticClass:"line-number"},[s._v("15")]),t("br"),t("span",{staticClass:"line-number"},[s._v("16")]),t("br"),t("span",{staticClass:"line-number"},[s._v("17")]),t("br"),t("span",{staticClass:"line-number"},[s._v("18")]),t("br"),t("span",{staticClass:"line-number"},[s._v("19")]),t("br"),t("span",{staticClass:"line-number"},[s._v("20")]),t("br"),t("span",{staticClass:"line-number"},[s._v("21")]),t("br"),t("span",{staticClass:"line-number"},[s._v("22")]),t("br"),t("span",{staticClass:"line-number"},[s._v("23")]),t("br"),t("span",{staticClass:"line-number"},[s._v("24")]),t("br"),t("span",{staticClass:"line-number"},[s._v("25")]),t("br"),t("span",{staticClass:"line-number"},[s._v("26")]),t("br"),t("span",{staticClass:"line-number"},[s._v("27")]),t("br"),t("span",{staticClass:"line-number"},[s._v("28")]),t("br"),t("span",{staticClass:"line-number"},[s._v("29")]),t("br"),t("span",{staticClass:"line-number"},[s._v("30")]),t("br"),t("span",{staticClass:"line-number"},[s._v("31")]),t("br"),t("span",{staticClass:"line-number"},[s._v("32")]),t("br"),t("span",{staticClass:"line-number"},[s._v("33")]),t("br"),t("span",{staticClass:"line-number"},[s._v("34")]),t("br"),t("span",{staticClass:"line-number"},[s._v("35")]),t("br"),t("span",{staticClass:"line-number"},[s._v("36")]),t("br"),t("span",{staticClass:"line-number"},[s._v("37")]),t("br"),t("span",{staticClass:"line-number"},[s._v("38")]),t("br"),t("span",{staticClass:"line-number"},[s._v("39")]),t("br"),t("span",{staticClass:"line-number"},[s._v("40")]),t("br"),t("span",{staticClass:"line-number"},[s._v("41")]),t("br"),t("span",{staticClass:"line-number"},[s._v("42")]),t("br"),t("span",{staticClass:"line-number"},[s._v("43")]),t("br"),t("span",{staticClass:"line-number"},[s._v("44")]),t("br"),t("span",{staticClass:"line-number"},[s._v("45")]),t("br"),t("span",{staticClass:"line-number"},[s._v("46")]),t("br"),t("span",{staticClass:"line-number"},[s._v("47")]),t("br"),t("span",{staticClass:"line-number"},[s._v("48")]),t("br"),t("span",{staticClass:"line-number"},[s._v("49")]),t("br"),t("span",{staticClass:"line-number"},[s._v("50")]),t("br"),t("span",{staticClass:"line-number"},[s._v("51")]),t("br"),t("span",{staticClass:"line-number"},[s._v("52")]),t("br"),t("span",{staticClass:"line-number"},[s._v("53")]),t("br"),t("span",{staticClass:"line-number"},[s._v("54")]),t("br"),t("span",{staticClass:"line-number"},[s._v("55")]),t("br"),t("span",{staticClass:"line-number"},[s._v("56")]),t("br"),t("span",{staticClass:"line-number"},[s._v("57")]),t("br"),t("span",{staticClass:"line-number"},[s._v("58")]),t("br"),t("span",{staticClass:"line-number"},[s._v("59")]),t("br"),t("span",{staticClass:"line-number"},[s._v("60")]),t("br"),t("span",{staticClass:"line-number"},[s._v("61")]),t("br"),t("span",{staticClass:"line-number"},[s._v("62")]),t("br"),t("span",{staticClass:"line-number"},[s._v("63")]),t("br"),t("span",{staticClass:"line-number"},[s._v("64")]),t("br"),t("span",{staticClass:"line-number"},[s._v("65")]),t("br"),t("span",{staticClass:"line-number"},[s._v("66")]),t("br"),t("span",{staticClass:"line-number"},[s._v("67")]),t("br"),t("span",{staticClass:"line-number"},[s._v("68")]),t("br"),t("span",{staticClass:"line-number"},[s._v("69")]),t("br"),t("span",{staticClass:"line-number"},[s._v("70")]),t("br"),t("span",{staticClass:"line-number"},[s._v("71")]),t("br"),t("span",{staticClass:"line-number"},[s._v("72")]),t("br"),t("span",{staticClass:"line-number"},[s._v("73")]),t("br"),t("span",{staticClass:"line-number"},[s._v("74")]),t("br"),t("span",{staticClass:"line-number"},[s._v("75")]),t("br"),t("span",{staticClass:"line-number"},[s._v("76")]),t("br"),t("span",{staticClass:"line-number"},[s._v("77")]),t("br"),t("span",{staticClass:"line-number"},[s._v("78")]),t("br")])])])}),[],!1,null,null,null);t.default=e.exports}}]);